{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI6Iuc7Z94TV3Pi5PePfQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pathin220/4105_Hw6/blob/main/Hw6_P1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "YUBQL7oEoVtg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/My Drive/Machine Learning/Housing.csv'\n",
        "housing = pd.DataFrame(pd.read_csv(file_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6C4Nz3RTfGS",
        "outputId": "8f2775c5-a8cd-4e1e-c1f8-7c5e0106c169"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "varlist =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
        "\n",
        "# Defining the map function\n",
        "def mapping(x):\n",
        "    return x.map({'yes': 1, 'no': 0, 'furnished':  1, 'semi-furnished':  0, 'unfurnished':  -1})\n",
        "\n",
        "housing[varlist] = housing[varlist].apply(mapping)"
      ],
      "metadata": {
        "id": "sJY_PX1quj7l"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the Dataset\n",
        "x_train, x_val = train_test_split(housing, train_size = 0.8, test_size = 0.2, random_state = 0)\n",
        "\n",
        "#Splitting Variables\n",
        "y_train = x_train.pop(\"price\")\n",
        "y_val = x_val.pop(\"price\")\n",
        "y_train = y_train.values.reshape(-1, 1)\n",
        "y_val = y_val.values.reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "gk_QjkeoAKf5"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standard = StandardScaler()\n",
        "vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'mainroad', 'guestroom', 'basement', 'hotwaterheating',\n",
        "         'airconditioning', 'prefarea', 'furnishingstatus']"
      ],
      "metadata": {
        "id": "NbCX78uDD8Ki"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1_train_stan = x_train\n",
        "x1_train_stan[vars] = standard.fit_transform(x1_train_stan[vars])\n",
        "\n",
        "x1_val_stan = x_val\n",
        "x1_val_stan[vars] = standard.fit_transform(x1_val_stan[vars])"
      ],
      "metadata": {
        "id": "mHxEUfn9D9ql"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stacking them there inputs\n",
        "x_train_stan_numpy = np.c_[np.ones((len(y_train), 1)), x1_train_stan[vars]]\n",
        "x_val_stan_numpy = np.c_[np.ones((len(y_val), 1)), x1_val_stan[vars]]\n",
        "\n",
        "\n",
        "x_train_ten = torch.tensor(x_train_stan_numpy, dtype=torch.float)\n",
        "x_val_ten = torch.tensor(x_val_stan_numpy, dtype=torch.float)\n",
        "y_train_ten = torch.tensor(y_train, dtype=torch.float)\n",
        "y_val_ten = torch.tensor(y_val, dtype=torch.float)"
      ],
      "metadata": {
        "id": "VyU9DnaXEQwr"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1a. Develop a Fully Connected Neural Network with only one hidden layer (size of 32) to predict the housing value for the housing dataset.\n",
        "#Make sure to include all input features. Compare your training loss value and validation results against the linear regression you implemented in Homework 5.\n",
        "#Can you compare your model complexity (number of trainable parameters) against linear regression? Note: Perform 20%, and 80% split for training and validation."
      ],
      "metadata": {
        "id": "A29BmjNKgtZ4"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one hidden layer\n",
        "seq_model_single = nn.Sequential(\n",
        "            nn.Linear(13, 32), # <1>\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 1)) # <2>\n",
        "seq_model_single"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "magkKNTpFeZd",
        "outputId": "f2a2a0f7-798c-4375-aefc-daecaff8c008"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=13, out_features=32, bias=True)\n",
              "  (1): Tanh()\n",
              "  (2): Linear(in_features=32, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, x_train, x_val,\n",
        "                  y_train, y_val):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        p_train = model(x_train) # <1>\n",
        "        loss_train = loss_fn(p_train, y_train,)\n",
        "\n",
        "        p_val = model(x_val) # <1>\n",
        "        loss_val = loss_fn(p_val, y_val)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_train.backward() # <2>\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch == 1 or epoch % 250 == 0:\n",
        "            print(f\"Epoch: {epoch}, Training loss: {loss_train.item():.4f},\"\n",
        "                  f\" Validation loss: {loss_val.item():.4f}\")"
      ],
      "metadata": {
        "id": "ZYqiynDgGFI4"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(seq_model_single.parameters(), lr=1e-2)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer=optimizer,\n",
        "    model = seq_model_single,\n",
        "    loss_fn = nn.MSELoss(),\n",
        "    x_train = x_train_ten,\n",
        "    x_val = x_val_ten,\n",
        "    y_train = y_train_ten,\n",
        "    y_val = y_val_ten\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKjQTJdbGnt_",
        "outputId": "a484821f-49f6-406f-bf1a-3efd917044b1"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 26469979914240.0000, Validation loss: 25189242896384.0000\n",
            "Epoch: 250, Training loss: 3640052154368.0000, Validation loss: 2884621565952.0000\n",
            "Epoch: 500, Training loss: 3639144873984.0000, Validation loss: 2884644372480.0000\n",
            "Epoch: 750, Training loss: 3638315188224.0000, Validation loss: 2884665868288.0000\n",
            "Epoch: 1000, Training loss: 3637555757056.0000, Validation loss: 2884686577664.0000\n",
            "Epoch: 1250, Training loss: 3636861075456.0000, Validation loss: 2884706500608.0000\n",
            "Epoch: 1500, Training loss: 3636225114112.0000, Validation loss: 2884725637120.0000\n",
            "Epoch: 1750, Training loss: 3635643678720.0000, Validation loss: 2884743725056.0000\n",
            "Epoch: 2000, Training loss: 3635111002112.0000, Validation loss: 2884761288704.0000\n",
            "Epoch: 2250, Training loss: 3634624200704.0000, Validation loss: 2884778590208.0000\n",
            "Epoch: 2500, Training loss: 3634178293760.0000, Validation loss: 2884794580992.0000\n",
            "Epoch: 2750, Training loss: 3633770659840.0000, Validation loss: 2884810571776.0000\n",
            "Epoch: 3000, Training loss: 3633397628928.0000, Validation loss: 2884825251840.0000\n",
            "Epoch: 3250, Training loss: 3633056055296.0000, Validation loss: 2884839407616.0000\n",
            "Epoch: 3500, Training loss: 3632743579648.0000, Validation loss: 2884853039104.0000\n",
            "Epoch: 3750, Training loss: 3632457580544.0000, Validation loss: 2884866408448.0000\n",
            "Epoch: 4000, Training loss: 3632195960832.0000, Validation loss: 2884878991360.0000\n",
            "Epoch: 4250, Training loss: 3631956623360.0000, Validation loss: 2884890525696.0000\n",
            "Epoch: 4500, Training loss: 3631737733120.0000, Validation loss: 2884902322176.0000\n",
            "Epoch: 4750, Training loss: 3631537192960.0000, Validation loss: 2884913332224.0000\n",
            "Epoch: 5000, Training loss: 3631353692160.0000, Validation loss: 2884924080128.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1b. We will increase the network complexity by adding two additional hidden layers, the hidden layers overall.\n",
        "#My suggestions for the size of layers are: 32, 64, 16, respectively.\n",
        "#Please redesign the network and compare your training loss value and validation results against the linear regression you implemented in Homework 5 and Problem 1.a.\n",
        "#Can you compare your model complexity? Note: Use the same 20%, and 80% split for training and validation."
      ],
      "metadata": {
        "id": "fiRKR79xHPqR"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#three hidden layer\n",
        "seq_model_multiple = nn.Sequential(\n",
        "            nn.Linear(13, 32), # <1>\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 64), # <2>\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 16), # <3>\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(16,1))\n",
        "seq_model_multiple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83YhPVjsH688",
        "outputId": "d772fb60-4e85-4207-8990-c6c6f1f2afc7"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=13, out_features=32, bias=True)\n",
              "  (1): Tanh()\n",
              "  (2): Linear(in_features=32, out_features=64, bias=True)\n",
              "  (3): Tanh()\n",
              "  (4): Linear(in_features=64, out_features=16, bias=True)\n",
              "  (5): Tanh()\n",
              "  (6): Linear(in_features=16, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer2 = optim.SGD(seq_model_multiple.parameters(), lr=1e-2)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer=optimizer2,\n",
        "    model = seq_model_multiple,\n",
        "    loss_fn = nn.MSELoss(),\n",
        "    x_train = x_train_ten,\n",
        "    x_val = x_val_ten,\n",
        "    y_train = y_train_ten,\n",
        "    y_val = y_val_ten\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qLQjZz9Is6k",
        "outputId": "8c568d7d-f091-45cd-bdb2-45d15f42e6d7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 26469982011392.0000, Validation loss: 25189244993536.0000\n",
            "Epoch: 250, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 500, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 750, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 1000, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 1250, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 1500, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 1750, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 2000, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 2250, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 2500, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 2750, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 3000, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 3250, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 3500, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 3750, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 4000, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 4250, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 4500, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 4750, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n",
            "Epoch: 5000, Training loss: 3644181970944.0000, Validation loss: 2884491542528.0000\n"
          ]
        }
      ]
    }
  ]
}